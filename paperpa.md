# 联邦学习数据异构问题的方法，收敛性以及展望

## 论文信息

- [ICML 2020] SCAFFOLD: Stochastic Controlled Averaging for Federated Learning
- [ICLR 2020] ON THE CONVERGENCE OF FEDAVG ON NON-IID DATA
- [ICDE 2022] Federated Learning on Non-IID Data Silos: An Experimental Study

## 背景介绍

联邦学习概念（FL, Federated Learning）最早由GOOGLE提出，用于保护一定隐私情况下的协同训练。一种联邦学习算法框架FedAvg[1]在2017年被B. McMahan等人提出，旨在增加本地节点的训练来减少通信次数。由于通信开销的大大减少，FedAvg被不断改进并被广泛应用。但是随着本地节点的训练次数增加，本地节点之间的数据异构带来的问题被放大。因此在数据异构的情况下，如何改进FedAvg以及其收敛性的分析成为一个热门话题。本文围绕联邦学习数据异构问题，从数据异构的方法，收敛性证明以及实验研究三个角度分别介绍一篇文章。

## 1. SCAFFOLD

如下图所示，传统的联邦学习算法在数据异构(non-iid)的情况下很容易产生client-drift的现象，即本地更新和全局更新的不一致。这种不一致会导致系统的收敛不稳定或者缓慢。该论文通过采用reduction variance的方法约束这种不一致。
![](图1)
### Method

SCAFFOLD算法对于本地训练的修改可以用下图表示。算法在本地和中心均初始化一个用于表示自己参数更新的变量c。本地变量和中心变量的差值即代表了client drift。本地在训练时用该差值来修正更新。
![](图2)

该算法主要有以下两个优点：

- 通过引入表征参数更新的变量，在本地训练的时候修正client drift。
- 由于client drift缩小，本地训练方向和全局训练方向更加靠近，大大减少了训练时间，从而节省了通信开销。

## 2. On The Convergence of FedAvg on Non-IID Data

该论文主要针对异构情况下对FedAvg收敛性做分析，且结果表明数据异构对收敛性有直接影响。本文针对convex情况以及节点全参与的联邦学习算法的收敛性做出展示。

### Assumptions

为了证明收敛性，文章定义了四个假设。第一个和第二个假设对本地损失函数做出了限制。第三个假设对SGD的梯度误差做出约束。第四个假设对本地训练的梯度做出约束。
![](图3)

除此之外文章还引入了一种间接衡量数据non-IID的程度，即全局最优与各本地最优的平均的差值Γ。
![](图4)

### Lemmas

通过上述假设，文章给出如下三个引理并在文章附件中给出相关证明。
![](图5)

### Convergence Analysis

通过上述引理，文章最终给出如下收敛式。其中的Γ为全局最优与各本地最优的平均的差值。该值一定程度上表示了数据non-IID的程度。通过式子，我们可以直接看出Γ的大小会影响FedAvg的收敛性。
![](图6)

## 3. An Experimental Study on Non-IID Data

该论文给出了若干种数据异构的场景，包括类别异构，特征异构，数量异构等。并且文章给出当前的一些主流方法在这些场景下的实验结果对比。通过对实验结果的分析，我们得出：极端的类别异构、划分更多的节点、节点拥有更少的数据、每轮只有部分节点参与训练等情景会给目前的联邦学习算法带来困扰。

### Label Distribution

如下表，文章通过将数据通过迪利克雷分布(β=0.5)或者将每个节点只有C个类别的数据的划分方式构造类别异构情景。结果表明大多数算法对于C=1的时候无能为力。
![](图7)

### Feature Distribution

如下表，文章通过加入高斯噪声，或者直接采用真实数据集等方法构造特征异构情景。结果表明差别不明显。该异构带来的影响有限。
![](图8)

### Quantity Distribution

如下表，文章通过对节点数量采用迪利克雷分布来构造样本数量异构的情景。结果表明FedAvg对该情景具有一定鲁棒性，而改良算法反而产生了退化。
![](图9)

### Larger Scale and Partially Participating

如下图，文章保持数据集不变，通过增加节点的数量来减少每个节点的私有数据，并且每轮迭代只有部分节点参与训练。结果表明，这种每轮只有部分节点参与的异构，以及节点样本数量的减少对联邦学习算法有明显的影响。
![](图10)

## 总结

通过实验和理论的双重证明，数据异构会很大程度上影响全局模型的整体效果。而且数据异构是一个实际应用中的常见问题，近年来有很多工作尝试缓解该问题。本文从一个有名方法SCAFOLLD出发，介绍如何引入本地和全局的更新变量来减少client drift从而更正本地节点的更新。随后本文介绍了数据异构情况下的理论验证和实验验证，得出对于目前的联邦学习算法，很多数据异构场景很是十分具有挑战。如何解决这种数据异构带来的收敛问题还需要后人的不断努力和尝试。


[1] Communication-Efficient Learning of Deep Networks from Decentralized Data.

